---
title: "Final_Assignment"
author: "Jacob Shore & Derek Huang"
date: "April 26, 2018"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width = 3.5, fig.height = 3.5, echo = TRUE)
library(dplyr)
library(ggplot2)
library(infer)
library(skimr)
library(broom)
library(readr)
options(digits=5)

library(readr)
kc_house_data <- read_csv("~/Downloads/kc_house_data.csv")
housing <- kc_house_data
set.seed(12897312)
housing2 <- housing[sample(nrow(housing), 1000),]
housing2 <- housing2 %>% dplyr::select(-c(id, date, yr_renovated, sqft_living15, sqft_lot15))
housing2$zipcode <- as.factor(housing2$zipcode)
```

# Introduction
In this study, we wish to analyze which factors best predict housing prices in King County (Seattle), WA. Our dataset describes over 21,000 homes sold between May 2014 and May 2015 in King County, so we anticipate the results are applicable to 2018 prices, as the economy has not shifted that much since 2015. The dataset consists of 19 variables: price, the number of bedrooms/bathrooms, the size of the house/parking lot/basement, the number of floors, whether waterfront or not, overall condition/grade, built year, renovation year, zip code and latitude/longitude coordinate. The data was taken from Kaggle, published by author "harlfoxem" under the title "House Sales in King County, WA." It is a well-known dataset on kaggle for performing regression analysis, but from what we can tell, our study is by far the most thorough.

Our primary task is to examine the relationship between price (response variable) and the various explanatory variables to see which are the most predictive. Some of the variables that we anticipated to be important predictors were square footage of living space, square footage of property, condition, view, longitude, and latitude (we were right on some of these!), since these are the variables that intutively seem like they should most affect the price. We have randomly selected 1,000 of the 21,613 observations to be included in our model analysis, as this sample size is enough to generalize to the full population, makes plotting results cleaner, and eases computational load.
 
The variabes will be occasionally referred to as follows in R output (note that some of them are factor variables with multiple levels):
sqft_living: the size of the house by square footage
sqft_lot: the size of the property by square footage
condition: appraiser's report on condition of the property, scale 1-5
view: appraiser's report on how nice the view is, scale 0-4
long: longitude of the house in degrees
lat: latitude of the house in degrees
bedrooms: number of bedrooms
bathrooms: number of bathrooms
waterfront: binary variable if the house is on the waterfront
grade: appraiser's report on grade of the house, King Co. specific
sqft_above: square footage of the attic (if applicable)
sqft_basement: square footage of the basement (if applicable)
yr_built: the year the house was constructed
zipcode: the zipcode of the property

Let us now review what we have accomplished so far. In the first assingment, we described our data in greater depth and gave summary statistics on some of the interesting variables. In the second assingment, we focused on the SLR model regressing price on square footage of living space, which we had a hunch would be the most interesting and best candidate for a linear model. Our procedure, including the eventual transformations of the variables, can be found in those notes. Moving on to MLR, we first determined that multicollinearity between variables was not a concern (see pairs plot from assingment 3). We then fit multiple models using many different procedures, testing for interaction, forward/backward selection methods, and BIC selection criteria to name a few. The results of this analysis can be found in those notes, and will be brought up again as we move forward with this summary.

# LASSO and RR

As a reminder, the final model from assignment 3 regressed the log of price on square footage of living space, latitude, view, condition, and interactions between square footage of living space and view, view and latitude, and square footage of living space and condition. This model will now be compared to the models from ridge regression and LASSO

```{r, echo=FALSE, warning=FALSE}
# Ridge Regression
require(ISLR); require(glmnet)
set.seed(232352)

housing2.RR <- housing2 %>% select(-price)
price.RR <- housing2$price

# set up lambda grid
lambda.grid =10^seq(7,-2, length =5000)

# build the model
housing.ridge <- glmnet(data.matrix(housing2.RR), price.RR, alpha=0,
lambda = lambda.grid, standardize=TRUE)

housing.ridge.cv <- cv.glmnet(data.matrix(housing2.RR), price.RR, alpha=0,
lambda = lambda.grid, standardize=TRUE)

plot(housing.ridge.cv)
abline(v=log(housing.ridge.cv$lambda.min), col="green")

colors <- rainbow(8)
plot(housing.ridge, xvar="lambda", xlim=c(-6,18),col=colors)
abline(v=log(housing.ridge.cv$lambda.min))
abline(h=0, lty=2)
text(rep(-6.5, 9), coef(housing.ridge)[-1,length(lambda.grid)], colnames(housing2.RR)[-9], pos=4, col=colors)

min.index <- which(lambda.grid == housing.ridge.cv$lambda.min)
ridge.coefs <- coef(housing.ridge)[,min.index]
ridge.coefs <- ridge.coefs[-1]
plot(ridge.coefs)
```
$\lambda$ was cross validated and the value that minimized cv error was found to be 10701, as shown in the first plot above (in log scale). The second plot shows the shrinkage of the coefficients as $\lambda$ increases. Finally, the third plot shows the coefficients of the 15 predictor variables under the value of $\lambda$ which minimized cv error. These coefficients are radically different --- the OLS model was very robust and involved a large number of variables, since there was interaction between continuous and factor variables. As such, a direct comparison of coefficients is difficult, although the positive and negative ones are preserved between the mdoels. We can say that ridge regression really elevated waterfront and longitude (at leasy didn't shrink these coefficients as much), and while latitude made it into the final OLS model, waterfront did not. We were also surprised that square footage of living space was shrunk so much in the ridge regression setting, as numerous OLS methods presented this variable as the most important! On to LASSO


```{r, echo=FALSE, warning=FALSE}
# LASSO
housing.lasso <- glmnet(data.matrix(housing2.RR), price.RR, alpha=1,
lambda = lambda.grid, standardize=TRUE)

housing.lasso.cv <- cv.glmnet(data.matrix(housing2.RR), price.RR, alpha=1,
lambda = lambda.grid, standardize=TRUE)

plot(housing.lasso.cv)
abline(v=log(housing.lasso.cv$lambda.min), col="green")

colors <- rainbow(8)
plot(housing.lasso, xvar="lambda", xlim=c(-6,18),col=colors)
abline(v=log(housing.lasso.cv$lambda.min))
abline(h=0, lty=2)
text(rep(-6.5, 9), coef(housing.lasso)[-1,length(lambda.grid)], colnames(housing2.RR)[-9], pos=4, col=colors)

min.index.las <- which(lambda.grid == housing.lasso.cv$lambda.min)
# lambda ~ .42 or index 77
lasso.coefs <- coef(housing.lasso)[,min.index.las]
lasso.coefs <- lasso.coefs[-1]
plot(lasso.coefs)
```

$\lambda$ was cross validated and the value that minimized cv error was found to be 657, as shown in the first plot above (in log scale). The second plot shows the shrinkage of the coefficients as $\lambda$ increases. Finally, the third plot shows the coefficients of the 15 predictor variables under the value of $\lambda$ which minimized cv error. Just like in RR, LASSO selected very similar coefficients. Surprisingly, only one variable was shrunk to 0 (square footage of basement space), and the rest of the coefficients were very similar to RR. Thus, the same curiosities arise in comparison to the OLS coefficients. 

Below, a pairs plot is presented for the three sets of coefficients. NOTE: a linear model was run on the full variable set in OLS, and this is NOT the same model as was compared above. Rather, the OLS model in the comparisons below contains no interaction terms, and is just the full explanatory variable set to match with RR/LASSO.

```{r, echo=FALSE, warning=FALSE}
lin.mod <- lm(price ~., data=housing2)
ols.coefs <- lin.mod$coefficients
ols.coefs <- ols.coefs[-1]
ols.coefs[is.na(ols.coefs)] <- 0
total.coefs <- data.frame(cbind(ols.coefs, ridge.coefs, lasso.coefs))

library(GGally)
ggpairs(total.coefs,
columns = c("ols.coefs", "ridge.coefs", "lasso.coefs"))
```

The pairs plot shows near identical selection of variables and coefficient values from RR and LASSO, while there is seemingly no correlation whatsoever with the OLS coefficients. These models are very, very different indeed.

```{r, echo=FALSE, warning=FALSE}
require(scales)
lasso.pred <-predict(housing.lasso, housing.lasso.cv$lambda.min, newx = data.matrix(housing2.RR), type="response")

RR.pred <-predict(housing.ridge, housing.ridge.cv$lambda.min, newx = data.matrix(housing2.RR), type="response")

ols.pred <- predict(full.model, newdata = housing2)
ols.pred <- exp(ols.pred)

ols.pred.bad <- predict(lin.mod, newdata = housing2)

plot(housing2$price, ols.pred, pch=16, col = alpha("black", .5), xlab = "Observed", ylab = "Predicted", xlim = c(0,3000000), ylim = c(0,3000000))
points(housing2$price, ols.pred.bad, pch=16, col = alpha("orange", .2))
points(housing2$price, RR.pred, pch=16, col = alpha("red", .2))
points(housing2$price, lasso.pred, pch=16, col = alpha("blue",.2))
legend(2100000,1100000,c("OLS(final)", "OLS(full)", "Ridge Reg.", "LASSO"), lty=c(1,1,1,1), lwd=c(2.5,2.5,2.5,2.5),col=c("black", "orange", "red", "blue"), cex = .62)

plot(housing2$price, ols.pred, pch=16, col = alpha("black", .5), xlab = "Observed", ylab = "Predicted", xlim = c(0,1000000), ylim = c(0,1000000))
points(housing2$price, ols.pred.bad, pch=16, col = alpha("orange", .2))
points(housing2$price, RR.pred, pch=16, col = alpha("red", .2))
points(housing2$price, lasso.pred, pch=16, col = alpha("blue",.2))
```

The above plots show the observed values of price on the x-axis, and the predicted values from all four models on the y-axis. Note that OLS(final) refers to the final model from last assignment, described above, while OLS(full) refers to the model on all expanatory variables with no interaction terms. Something to note --- even though all the models are pretty different in makeup of coefficients, all models predict relatively well (and relatively the same). LASSO/RR predictions are nearly identical for the reasons described above. One thing that is particularly interesting is that while all predictions are pretty solid, all models besides the final model from have a slight bend in their slopes, predicting too low for small values and slightly too high for middle values. This is particularly noticeable in the "heart" of the prediction zone between 0 and 1 million dollar homes. The final model is distinctively the most linear between observed and predicted --- I guess we did alright in the last assignment!

# Splines and LOESS

```{r, echo=FALSE, warning=FALSE}
# Splines
require(splines)

price.rs <- lm(price ~ bs(sqft_living, df=4, degree=3), data=housing2) 
# summary(price.rs)
distance.range <- range(housing2$sqft_living)
sqft_living.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

price.rs.pred <- predict(price.rs, newdata = data.frame(sqft_living=sqft_living.grid), se=TRUE)


plot(housing2$sqft_living, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Square Footage of Living Space", ylab="Price")
title("Cubic Regression Splines on K Knots",outer =F)

knots <- c(3,5,10,40)
color = 0
SSE.vec.rs <- c()
for(i in knots){
  var.degree <- lm(price ~ bs(sqft_living, df=i, degree=3), data=housing2) 
  SSE <- sum(var.degree$residuals^2)
  SSE.vec.rs[i] <- SSE
  var.pred <- predict(var.degree, newdata= data.frame(sqft_living=sqft_living.grid), se=TRUE)
  color = color+1
  lines(sqft_living.grid, var.pred$fit, lwd=2, col =color)
}
legend(6000,2000000,c(3,5,10,40), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "# Knots")
```

```{r}
# LOESS
distance.range <- range(housing2$sqft_living)
sqft_living.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

plot(housing2$sqft_living, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Square Footage of Living Space", ylab="Price")
title("Local Regression (loess)",outer =F)

span <- c(.1,.3,.6,.9)
color = 0
SSE.vec.lor <- c()
for(i in span){
price.lor <- loess(price ~ sqft_living, span=i, data=housing2)
SSE <- sum(price.lor$residuals^2)
  SSE.vec.lor[i*10] <- SSE
price.lor.pred <- predict(price.lor, newdata = data.frame(sqft_living=sqft_living.grid), se=TRUE)
color=color+1
lines(sqft_living.grid, price.lor.pred$fit ,lwd =2, col =color)
}
legend(6000,2000000,c(.1,.3,.6,.9), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "Span")


# CV is .9
```

As we've discussed ad nauseum, regressing price on square footage of living space is already a pretty solid linear model without any additional changes. Thus, the smallest number of knots and the largest span seem to give the best estimated regression functions. We'd give a slight edge to the spline model, since prediction intervals are slightly easier to come by / to interpret, and it is slightly less computationally expensive. But these aren't all that interesting, since square footage of living space is already a really solid single variable predictor of price. Let's look at a variable which has less of a classic linear relationship with price but still has some interesting predictive properties --- latitude. In previous assignments, latitude has always shown up as a significant predictor, but the relationship between latitude and price is not that straightforward.

```{r}
# Splines
price.rs <- lm(price ~ bs(lat, df=4, degree=3), data=housing2) 
# summary(price.rs)
distance.range <- range(housing2$lat)
lat.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

price.rs.pred <- predict(price.rs, newdata = data.frame(lat=lat.grid), se=TRUE)


plot(housing2$lat, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Latitude", ylab="Price")
title("Cubic Regression Spline on K Knots",outer =F)

knots <- c(3,5,10,40)
color = 0
SSE.vec.rs <- c()
for(i in knots){
  var.degree <- lm(price ~ bs(lat, df=i, degree=3), data=housing2) 
  SSE <- sum(var.degree$residuals^2)
  SSE.vec.rs[i] <- SSE
  var.pred <- predict(var.degree, newdata= data.frame(lat=lat.grid), se=TRUE)
  color = color+1
  lines(lat.grid, var.pred$fit, lwd=2, col =color)
}
legend(47.2,3000000,c(3,5,10,40), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "# Knots")
```

```{r}
# LOESS
distance.range <- range(housing2$lat)
lat.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

plot(housing2$lat, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Latitude", ylab="Price")
title("Local Regression (loess)",outer =F)

span <- c(.1,.3,.6,.9)
color = 0
SSE.vec.lor <- c()
for(i in span){
price.lor <- loess(price ~ lat, span=i, data=housing2)
SSE <- sum(price.lor$residuals^2)
  SSE.vec.lor[i*10] <- SSE
price.lor.pred <- predict(price.lor, newdata = data.frame(lat=lat.grid), se=TRUE)
color=color+1
lines(lat.grid, price.lor.pred$fit ,lwd =2, col =color)
}
legend(47.2,3000000,c(.1,.3,.6,.9), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "Span")
```

Again, both styles provide some good fits in the middle ranges of their respective parameters to a relationship which is clearly non-linear (the downtown area has a spike in prices). We give a slight edge to LOESS in this case (on a span of .3) for very nice smoothness and easy interpretability. The behavior towards the fringes of the latitude range is particularly better in the LOESS model. Also, we cheated and cross-validated, and this model had the lowest cv error (we would have picked it anyways!). All in all, none of these models would suggest a strong relationship between latitude and price, of any form. 

While none of the models presented above are "bad" fits per se, it is apparent that the work we did in MLR / OLS is better suited to this dataset. We have seen in homework how powerful these methods are in dealing with multicollinearity and non-linearity, but sadly (or happily?) these concerns do not burden us with this data. For the shrinkage models, sacrificing bias to decrease the variance when the variance is already very low just doesn't make sense. Furthermore, while the relationship between some of the other variable (besides sq. ft. living space) and price is interesting, none of them follow a shape that smoothing techniques really "apply" to. Thus, we are comfortable with our results from the last assignment, and would not alter that final model in any way shown above. Moving forward, something that we think could be very interesting is to look at demographic data, as this was not included in our dataset. Data about race, class, and age is out there, and in a rapidly changing city like Seattle could be a predictive category with interesting results. 

# Normal Probability Plots

Perhaps the least talked-about technical assumption is normality of error terms. While slight deviations from normality usually do not pose issues, and the magic of the Central Limit Theorem suggests that even larger deviations from normality vanish under large sample sizes, normaly of error terms plays a major role in the theory underlying prediction intervals. In fact, the width of these intervals (specifically the multipliers) is a direct result of normal theory --- the lack of normality means we are lost when searching for the correct multiplier (assuming the distribution of errors is unknown). Thus, even if the regression fit is solid, it may be worthwhile to check normality of errors --- prediction intervals may not be as trustworthy as they seem. And in the realm of housing prices, prediction intervals are extremely important --- it is common for someone to want to know what the price range of houses is for a given set of predictors. For instance, a family which needs a house with 3 bedrooms, at Z latitude, and with W square feet of living space needs to have a price range to adequately compare to similar houses available in another location.

The prevailing method for checking normality of errors is the normal probability plot, a special subtype of probability plot. In a normal probability plot (or qqplot), the residuals are plotted against their expected value under normality. That is, how likely a residual of that magnitude would be under the assumption that the residuals are normally distributed. The plot reads "quantiles," since this process implicitly compares the quantiles of the data (residuals) to the quantiles of a corresponding normal distribution (scaled by MSE). Thus, a plot that is linear suggests a normal distribution of errors, since each residual is nearly its expected value under the normality assumption. Furthermore, we can tell the exact manner in which the errors are distributed from this plot. For example, data which is skewed right would show up with a slope less than 1 for lower ordinal normal statistics (the x-axis) and greater than 1 for higher ordinal normal statistics. Data which has fat tails would show up at linear in the middle of the graph, but deviate to the bottom and top of the linear fit for low and high ordinal normal statistics, respectively. 

The function which maps residuals to their expected value under normality has been found to be (k-.375/n+.25)*sqrt(MSE), where k is the kth smallest residual, and n is the total number of residuals. Since sqrt(MSE) is simply a scaling factor here, it may be omitted in the calculation without altering the shape of the plot. Let's move on to some examples for our data. As standardized residuals are simply another scaling factor, they to do not affect the nature of the plots, and we will use those as our input in the plots below.

```{r, echo=FALSE}
housing_lm <- lm(price ~ sqft_living, data=housing2)
# untransformed model
augment(housing_lm) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")
qqnorm(housing_lm$residuals)
```
The above Q-Q plot is for the SLR model where price is predicted solely from square footage of living space. As shown by the non-linearity, normality of errors is not a great assumption to make here. The standardized residual plot shows many issues here on top of normality (it is severely heteroskedastic). In assignment 2, we used a power model (log transformation on both x and y) to get around these issues. Let's see what the Q-Q plot looks like in that arena.

```{r, echo=FALSE}
housing_lm3 <- lm(log(price) ~ log(sqft_living), data=housing2)
# transforming both X and Y using log (power model)
augment(housing_lm3) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")
# power model residual
qqnorm(housing_lm3$residuals)
```
Much better! The power model fixed the issues with the variance, but simultaneously fixed the issues with normality of errors. This is the model we'd move forward with in the SLR setting. Now let's check the final model we presented in the MLR setting, which regressed the log of price on square footage of living space, latitude, view, condition, and interactions between square footage of living space and view, view and latitude, and square footage of living space and condition.
```{r, echo=FALSE, warning=FALSE}
full.model <- lm(log(price) ~ (sqft_living + as.factor(condition) + as.factor(view) + lat + sqft_living*as.factor(condition) + sqft_living*as.factor(view) + as.factor(view)*lat), data=housing2)

augment(full.model) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")

qqnorm(full.model$residuals)
```
Again, there is nothing to worry about here. The Q-Q plot shows slight bias towards fat tails, but that's why we use the t-distribution anyways! We should note that we weren't expecting a major deviation from normality in an of these scenarios --- our sample size was large (1000) and the transformations made the linear model very appropriate.

# Inverse Predictions

Inverse predictions refer to the situation when a new value appears on the response, and this value is used to infer what the value(s) of the predictors are using the existing regression of Y on X. In the housing market, situtations like this arise all the time. It is not uncommon for potential buyers to browse by price (Y) only at first, and having a tool which can build out the details of the house would be very valuable (obviously this information would typically exist, but maybe the buyer is extremely lazy).

Regression models are of course deterministic in nature, and so inverses are easy to come by. Note that the inverse prediction is NOT attained by regressing X on Y (called inverse regression), as this solves a different problem entirely (albeit with a similar result in most cases). The estimated regression function is assumed to be as always:

$\hat{Y} = b_0 + b_1*X$

If we solve this equation for X given Y_new, we get the form

$\hat{X}_{new} = \frac{(Y_{new} - b_0)}{b_1}$

which is the MLE for $X_{new}$ assuming that there is a linear relationship between X and Y ($b_1$ cannot be 0). Assuming normality of errors and constant variance as before grants the confidence interval:

$\hat{X}_{new} \pm t(1 - \frac{\alpha}{2}; n-2)s\{predX\}$

where

$s\{predX\} = \frac{MSE}{b_1^2} [1 + \frac{1}{n} + (\frac{(\hat{X}_{new} - \bar{X})^2}{\sum (X_i - \bar{X})^2})$

And that's it. If we can safely conclude that $\beta_1$ is NOT 0, then we are good to go with the inverse predictions. Note that the given interval above relies on normal theory --- an exact confidence interval may be obtained through some tedious algebra. It will not be reproduced here, but may be found at https://journal.r-project.org/archive/2014/RJ-2014-009/RJ-2014-009.pdf in the documentation for the "investr" package, which we will employ to get inversion intervals.

Furthermore, in the case of multiple new observations, a Bonferroni or Scheffe procedure may be specified to adjust the critical values of the intervals accordingly. This will be illustrated below. 

The model we will use is the same as above, where the log of price is regressed on the log of square footage of living space. The "investr" package was used to compute exact confidence intervals. This package automatically calculates the slope estimate, and performs a test of significance to make sure there is a linear relationship present. First, we will attempt to estimate what level of square footage of housing space led to a house listed for 500,000 dollars.

```{r, warning=FALSE, echo=FALSE}
library(investr)
housing_lm3 <- lm(log(price) ~ log(sqft_living), data=housing2)
res <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90)
cat("A 95% CI for the square footage of living space of a home listed for 500,000 dollars in King County is (",exp(res$lower),",",exp(res$upper),") sq. ft.")


ggplot(housing_pred, aes(x = exp(log.sqft_living.), y = exp(log.price.))) + geom_point(alpha = .5, col = 'black') +
stat_smooth(method = "lm", se = FALSE) + geom_hline(yintercept =  500000, lty = 2)+geom_vline(xintercept = c(exp(res$lower), exp(res$estimate), exp(res$upper)), lty = 2)+xlim(0,6000)+labs(x = "Square Footage of Living Space", y = "Price")
```

In the plot above, the shaded area represents the confidence interval around the fit of the regression function, while the dashed lines represent the new observation, the best guess for what value of square footage of living space corresponds to a 500,000 dollar house, and the inversion intervals for that guess at 90% confidence. Recall that the regression model was fit on the log of both predictor and response, so they have been back-transformed here.

```{r, echo=FALSE, warning=FALSE}
res2 <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90, adjust = "Scheffe", k = 5)
res3 <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90, adjust = "Bonferroni", k = 5)

ggplot(housing_pred, aes(x = exp(log.sqft_living.), y = exp(log.price.))) + geom_point(alpha = .5, col = 'black') +
stat_smooth(method = "lm", se = FALSE)+geom_hline(yintercept =  500000, lty = 2)+xlim(0,9000)+labs(x = "Square Footage of Living Space", y = "Price")+geom_vline(xintercept = c(exp(res2$lower), exp(res2$estimate), exp(res2$upper)), lty = 2, col="blue")+geom_vline(xintercept = c(exp(res3$lower), exp(res3$estimate), exp(res3$upper)), lty = 2, col = "red")+geom_vline(xintercept = c(exp(res$lower), exp(res$estimate), exp(res$upper)), lty = 2)
```

The above plot shows simultaneous inference for 5 new houses that entered the market at 500,000. In red are the Scheffe bounds, and in blue are the Bonferroni bounds for the inversion intervals. Note that all intervals here are big --- perhaps too big? We cannot determine if there is an error somewhere within the package or if the inversion intervals for simultaneous inference really are that large. Either way, nobody is going to be getting any information out of knowing that a house listed for 500,000 dollars is between 1000 and 45000 square feet.

# Generalized Additive Models
GAMs provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. GAMs are extraordinarily flexible, as they can be applied with both quantitative and qualitative responses.

An example of GAM would be, instead of having $y_i = \beta_0 + \beta_1 x_{i1}+ \beta_2x_{i2} + ... \beta_p x_{ip} + \epsilon_i$, we would write the model as  $y_i = \beta_0 + f_1(x_{i1})+ f_2(x_{i2}) + .... f_p(x_{ip}) + \epsilon_i$. It shows the addictive characteristic because we calculate a separate $f_j$ for each $X_j$, and then add together all of their contributions.

We picked GAMs because GAMs allow us to fit a non-linear $f_j$ to each $X_j$, so that we can automatically model non-linear relationships that standard linear regression will miss. And the non-linear fits can potentially make more accurate predictions. For example, for latitude, GAMs might yield better fit for us. 
Also, since the model is additive, we can still examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed. Hence if we are interested in inference, GAMs provide a useful representation.


```{r}
library(mgcv)
library(gam)
# fit the model using natural splines
gam1=lm(price ~ ns(sqft_living,4)+ns(lat,5)+ns(condition, 5),data=housing)
summary(gam1)
<<<<<<< HEAD
#plot(gam1)
par(mfrow=c(1,2))
plot(gam1, se=TRUE,col="black")
=======
par(mfrow=c(1,3))
plot.gam(gam1, se=TRUE,col="blue")
>>>>>>> d60c76de1bbfa6ac1669086237ed7ab41a7f3268

# fit the model using smoothing splines 
gam2=lm(price ~ s(sqft_living,4)+s(lat,5)+s(condition, 5),data=housing)
par(mfrow=c(1,3))
plot.gam(gam2, se=TRUE,col="blue")

# GAM that uses a linear function of latitude 
gam3=lm(price ~ ns(sqft_living,4)+lat +ns(condition, 5),data=housing)
# GAM that excludes latitude 
gam4=lm(price ~ ns(sqft_living,4)+ns(condition, 5),data=housing)
anova(gam1,gam3,gam4,test="F")
```
Each plot displays the fitted function and pointwise standard errors.
As we can see, there is a positive relationship between square feet living/price and condition/price. And houses with a latitude of 47.7 is more expensive. 
From the f-test we find that there is compelling evidence that a GAM with a non-linear function of latitude is better than a GAM with a linear function of latitude or one that does not latitude at all (p-value ~ 0). And this confirms our conjecture that non-linear fits could be better for latitude.

# Summary

In this study, we examined which factors best predict housing prices in King County (Seattle), WA. At the beginning of the experiment, we were confident in being able to fit a linear model well, as it is well-known that housing prices are affected by some of the variables we had access to (such as square footage of living space, number of bedrooms, etc.). One aspect of the pricing dynamics that we hoped to uncover was whether objective (square footage, rooms, location) or subjective (appraisal scores on condition, view) carried more weight in the model. As it turns out, square footage of living space is unsurprisingly the strongest predictor, as it had a near perfect linear fit with the log of price, it was the most significant predictor in the MLR setting, and it was consistently the first variable in the model in forward / backward selection methods. Some of the appraisal scores were also very significant predictors --- specifically the view rating and condition rating. Surprisingly, latitude ended up being a very significant predictor across all of the model types we analyzed, while longitude was not. Perhaps even more surprisingly, predictors which we thought might be important, such as square footage of property and appraisal ratings on grade and objective measures such as number of bathrooms and bedrooms were not as relevant as the variables listed previously. That is not to say they didn't have predictive power --- they did, but the other variables were so noteworthy that adding in these additional variables was not worth the marginal benefit at the risk of overfitting. 

As stated above, shrinkage and smoothing models did not provide any new insight in terms of model building, and confirmed our hunch that the final MLR model we constructed in assignment 3 was the best for our situation. Our data was staunchly linear across many of the variables,

We started by doing ....

Final model on X varaibles, p-val, R2, other stuff. ANOVA test... interaction WAS sig. Coef estimates. Reasons for the variables in this model.

shrinkage and smoothing models did not provide anything interesting, and confirmed why we did MLR

Housing prices change RAPIDLY, and vary across cities.