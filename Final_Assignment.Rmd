---
title: "Final_Assignment"
author: "Jacob Shore & Derek Huang"
date: "April 26, 2018"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width = 3.5, fig.height = 3.5, echo = TRUE)
library(dplyr)
library(ggplot2)
library(infer)
library(skimr)
library(broom)
library(readr)
options(digits=5)

library(readr)
kc_house_data <- read_csv("~/Downloads/kc_house_data.csv")
housing <- kc_house_data
set.seed(12897312)
housing2 <- housing[sample(nrow(housing), 1000),]
housing2 <- housing2 %>% dplyr::select(-c(id, date, yr_renovated, sqft_living15, sqft_lot15))
housing2$zipcode <- as.factor(housing2$zipcode)
```

# Introduction
Our dataset describes over 21,000 homes sold between May 2014 and May 2015 in King County, WA (Seattle area). The dataset consists of 19 variables: price, the number of bedrooms/bathrooms, the size of the house/parking lot/basement, the number of floors, whether waterfront or not, overall condition/grade, built year, renovation year, zip code and latitude/longitude coordinate. We have randomly selected 1,000 of the 21,613 observations to be included in our model analysis.

We are most interested in the relationship between price (response vairable) and the following explanatory variables: square footage of living space, square footage of property, condition, view, longitude, and latitude because these are the variables that intutively seem like they should most affect the price.


# LASSO and RR

As a reminder, the final model from assignment 3 regressed the log of price on square footage of living space, latitude, view, condition, and interactions between square footage of living space and view, view and latitude, and square footage of living space and condition. This model will now be compared to the models from ridge regression and LASSO

```{r, echo=FALSE, warning=FALSE}
# Ridge Regression
require(ISLR); require(glmnet)
set.seed(232352)

housing2.RR <- housing2 %>% select(-price)
price.RR <- housing2$price

# set up lambda grid
lambda.grid =10^seq(7,-2, length =5000)

# build the model
housing.ridge <- glmnet(data.matrix(housing2.RR), price.RR, alpha=0,
lambda = lambda.grid, standardize=TRUE)

housing.ridge.cv <- cv.glmnet(data.matrix(housing2.RR), price.RR, alpha=0,
lambda = lambda.grid, standardize=TRUE)

plot(housing.ridge.cv)
abline(v=log(housing.ridge.cv$lambda.min), col="green")

colors <- rainbow(8)
plot(housing.ridge, xvar="lambda", xlim=c(-6,18),col=colors)
abline(v=log(housing.ridge.cv$lambda.min))
abline(h=0, lty=2)
text(rep(-6.5, 9), coef(housing.ridge)[-1,length(lambda.grid)], colnames(housing2.RR)[-9], pos=4, col=colors)

min.index <- which(lambda.grid == housing.ridge.cv$lambda.min)
ridge.coefs <- coef(housing.ridge)[,min.index]
ridge.coefs <- ridge.coefs[-1]
plot(ridge.coefs)
```
$\lambda$ was cross validated and the value that minimized cv error was found to be 10701, as shown in the first plot above (in log scale). The second plot shows the shrinkage of the coefficients as $\lambda$ increases. Finally, the third plot shows the coefficients of the 15 predictor variables under the value of $\lambda$ which minimized cv error. These coefficients are radically different --- the OLS model was very robust and involved a large number of variables, since there was interaction between continuous and factor variables. As such, a direct comparison of coefficients is difficult, although the positive and negative ones are preserved between the mdoels. We can say that ridge regression really elevated waterfront and longitude (at leasy didn't shrink these coefficients as much), and while latitude made it into the final OLS model, waterfront did not. We were also surprised that square footage of living space was shrunk so much in the ridge regression setting, as numerous OLS methods presented this variable as the most important! On to LASSO


```{r, echo=FALSE, warning=FALSE}
# LASSO
housing.lasso <- glmnet(data.matrix(housing2.RR), price.RR, alpha=1,
lambda = lambda.grid, standardize=TRUE)

housing.lasso.cv <- cv.glmnet(data.matrix(housing2.RR), price.RR, alpha=1,
lambda = lambda.grid, standardize=TRUE)

plot(housing.lasso.cv)
abline(v=log(housing.lasso.cv$lambda.min), col="green")

colors <- rainbow(8)
plot(housing.lasso, xvar="lambda", xlim=c(-6,18),col=colors)
abline(v=log(housing.lasso.cv$lambda.min))
abline(h=0, lty=2)
text(rep(-6.5, 9), coef(housing.lasso)[-1,length(lambda.grid)], colnames(housing2.RR)[-9], pos=4, col=colors)

min.index.las <- which(lambda.grid == housing.lasso.cv$lambda.min)
# lambda ~ .42 or index 77
lasso.coefs <- coef(housing.lasso)[,min.index.las]
lasso.coefs <- lasso.coefs[-1]
plot(lasso.coefs)
```

$\lambda$ was cross validated and the value that minimized cv error was found to be 657, as shown in the first plot above (in log scale). The second plot shows the shrinkage of the coefficients as $\lambda$ increases. Finally, the third plot shows the coefficients of the 15 predictor variables under the value of $\lambda$ which minimized cv error. Just like in RR, LASSO selected very similar coefficients. Surprisingly, only one variable was shrunk to 0 (square footage of basement space), and the rest of the coefficients were very similar to RR. Thus, the same curiosities arise in comparison to the OLS coefficients. 

Below, a pairs plot is presented for the three sets of coefficients. NOTE: a linear model was run on the full variable set in OLS, and this is NOT the same model as was compared above. Rather, the OLS model in the comparisons below contains no interaction terms, and is just the full explanatory variable set to match with RR/LASSO.

```{r, echo=FALSE, warning=FALSE}
lin.mod <- lm(price ~., data=housing2)
ols.coefs <- lin.mod$coefficients
ols.coefs <- ols.coefs[-1]
ols.coefs[is.na(ols.coefs)] <- 0
total.coefs <- data.frame(cbind(ols.coefs, ridge.coefs, lasso.coefs))

library(GGally)
ggpairs(total.coefs,
columns = c("ols.coefs", "ridge.coefs", "lasso.coefs"))
```

The pairs plot shows near identical selection of variables and coefficient values from RR and LASSO, while there is seemingly no correlation whatsoever with the OLS coefficients. These models are very, very different indeed.

```{r, echo=FALSE, warning=FALSE}
require(scales)
lasso.pred <-predict(housing.lasso, housing.lasso.cv$lambda.min, newx = data.matrix(housing2.RR), type="response")

RR.pred <-predict(housing.ridge, housing.ridge.cv$lambda.min, newx = data.matrix(housing2.RR), type="response")

ols.pred <- predict(full.model, newdata = housing2)
ols.pred <- exp(ols.pred)

ols.pred.bad <- predict(lin.mod, newdata = housing2)

plot(housing2$price, ols.pred, pch=16, col = alpha("black", .5), xlab = "Observed", ylab = "Predicted", xlim = c(0,3000000), ylim = c(0,3000000))
points(housing2$price, ols.pred.bad, pch=16, col = alpha("orange", .2))
points(housing2$price, RR.pred, pch=16, col = alpha("red", .2))
points(housing2$price, lasso.pred, pch=16, col = alpha("blue",.2))
legend(2100000,1100000,c("OLS(final)", "OLS(full)", "Ridge Reg.", "LASSO"), lty=c(1,1,1,1), lwd=c(2.5,2.5,2.5,2.5),col=c("black", "orange", "red", "blue"), cex = .62)

plot(housing2$price, ols.pred, pch=16, col = alpha("black", .5), xlab = "Observed", ylab = "Predicted", xlim = c(0,1000000), ylim = c(0,1000000))
points(housing2$price, ols.pred.bad, pch=16, col = alpha("orange", .2))
points(housing2$price, RR.pred, pch=16, col = alpha("red", .2))
points(housing2$price, lasso.pred, pch=16, col = alpha("blue",.2))
```

The above plots show the observed values of price on the x-axis, and the predicted values from all four models on the y-axis. Note that OLS(final) refers to the final model from last assignment, described above, while OLS(full) refers to the model on all expanatory variables with no interaction terms. Something to note --- even though all the models are pretty different in makeup of coefficients, all models predict relatively well (and relatively the same). LASSO/RR predictions are nearly identical for the reasons described above. One thing that is particularly interesting is that while all predictions are pretty solid, all models besides the final model from have a slight bend in their slopes, predicting too low for small values and slightly too high for middle values. This is particularly noticeable in the "heart" of the prediction zone between 0 and 1 million dollar homes. The final model is distinctively the most linear between observed and predicted --- I guess we did alright in the last assignment!

# Splines and LOESS

```{r, echo=FALSE, warning=FALSE}
# Splines
require(splines)

price.rs <- lm(price ~ bs(sqft_living, df=4, degree=3), data=housing2) 
# summary(price.rs)
distance.range <- range(housing2$sqft_living)
sqft_living.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

price.rs.pred <- predict(price.rs, newdata = data.frame(sqft_living=sqft_living.grid), se=TRUE)


plot(housing2$sqft_living, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Square Footage of Living Space", ylab="Price")
title("Cubic Regression Splines on K Knots",outer =F)

knots <- c(3,5,10,40)
color = 0
SSE.vec.rs <- c()
for(i in knots){
  var.degree <- lm(price ~ bs(sqft_living, df=i, degree=3), data=housing2) 
  SSE <- sum(var.degree$residuals^2)
  SSE.vec.rs[i] <- SSE
  var.pred <- predict(var.degree, newdata= data.frame(sqft_living=sqft_living.grid), se=TRUE)
  color = color+1
  lines(sqft_living.grid, var.pred$fit, lwd=2, col =color)
}
legend(6000,2000000,c(3,5,10,40), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "# Knots")
```

```{r}
# LOESS
distance.range <- range(housing2$sqft_living)
sqft_living.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

plot(housing2$sqft_living, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Square Footage of Living Space", ylab="Price")
title("Local Regression (loess)",outer =F)

span <- c(.1,.3,.6,.9)
color = 0
SSE.vec.lor <- c()
for(i in span){
price.lor <- loess(price ~ sqft_living, span=i, data=housing2)
SSE <- sum(price.lor$residuals^2)
  SSE.vec.lor[i*10] <- SSE
price.lor.pred <- predict(price.lor, newdata = data.frame(sqft_living=sqft_living.grid), se=TRUE)
color=color+1
lines(sqft_living.grid, price.lor.pred$fit ,lwd =2, col =color)
}
legend(6000,2000000,c(.1,.3,.6,.9), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "Span")


# CV is .9
```

As we've discussed ad nauseum, regressing price on square footage of living space is already a pretty solid linear model without any additional changes. Thus, the smallest number of knots and the largest span seem to give the best estimated regression functions. We'd give a slight edge to the spline model, since prediction intervals are slightly easier to come by / to interpret, and it is slightly less computationally expensive. But these aren't all that interesting, since square footage of living space is already a really solid single variable predictor of price. Let's look at a variable which has less of a classic linear relationship with price but still has some interesting predictive properties --- latitude. In previous assignments, latitude has always shown up as a significant predictor, but the relationship between latitude and price is not that straightforward.

```{r}
# Splines
price.rs <- lm(price ~ bs(lat, df=4, degree=3), data=housing2) 
# summary(price.rs)
distance.range <- range(housing2$lat)
lat.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

price.rs.pred <- predict(price.rs, newdata = data.frame(lat=lat.grid), se=TRUE)


plot(housing2$lat, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Latitude", ylab="Price")
title("Cubic Regression Spline on K Knots",outer =F)

knots <- c(3,5,10,40)
color = 0
SSE.vec.rs <- c()
for(i in knots){
  var.degree <- lm(price ~ bs(lat, df=i, degree=3), data=housing2) 
  SSE <- sum(var.degree$residuals^2)
  SSE.vec.rs[i] <- SSE
  var.pred <- predict(var.degree, newdata= data.frame(lat=lat.grid), se=TRUE)
  color = color+1
  lines(lat.grid, var.pred$fit, lwd=2, col =color)
}
legend(47.2,3000000,c(3,5,10,40), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "# Knots")
```

```{r}
# LOESS
distance.range <- range(housing2$lat)
lat.grid <- seq(from= distance.range[1], to=distance.range[2], length.out = 50)

plot(housing2$lat, housing2$price ,xlim=distance.range ,cex =.5, pch=19,
col =" grey40 ", xlab="Latitude", ylab="Price")
title("Local Regression (loess)",outer =F)

span <- c(.1,.3,.6,.9)
color = 0
SSE.vec.lor <- c()
for(i in span){
price.lor <- loess(price ~ lat, span=i, data=housing2)
SSE <- sum(price.lor$residuals^2)
  SSE.vec.lor[i*10] <- SSE
price.lor.pred <- predict(price.lor, newdata = data.frame(lat=lat.grid), se=TRUE)
color=color+1
lines(lat.grid, price.lor.pred$fit ,lwd =2, col =color)
}
legend(47.2,3000000,c(.1,.3,.6,.9), lty=c(1,1), lwd=c(2.5,2.5),col=c(1,2,3,4), title = "Span")
```

Again, both styles provide some good fits in the middle ranges of their respective parameters to a relationship which is clearly non-linear (the downtown area has a spike in prices). We give a slight edge to LOESS in this case (on a span of .3) for very nice smoothness and easy interpretability. The behavior towards the fringes of the latitude range is particularly better in the LOESS model. Also, we cheated and cross-validated, and this model had the lowest cv error (we would have picked it anyways!). All in all, none of these models would suggest a strong relationship between latitude and price, of any form. 

While none of the models presented above are "bad" fits per se, it is apparent that the work we did in MLR / OLS is better suited to this dataset. We have seen in homework how powerful these methods are in dealing with multicollinearity and non-linearity, but sadly (or happily?) these concerns do not burden us with this data. For the shrinkage models, sacrificing bias to decrease the variance when the variance is already very low just doesn't make sense. Furthermore, while the relationship between some of the other variable (besides sq. ft. living space) and price is interesting, none of them follow a shape that smoothing techniques really "apply" to. Thus, we are comfortable with our results from the last assignment, and would not alter that final model in any way shown above. Moving forward, something that we think could be very interesting is to look at demographic data, as this was not included in our dataset. Data about race, class, and age is out there, and in a rapidly changing city like Seattle could be a predictive category with interesting results. 

# Normal Probability Plots

Perhaps the least talked-about technical assumption is normality of error terms. While slight deviations from normality usually do not pose issues, and the magic of the Central Limit Theorem suggests that even larger deviations from normality vanish under large sample sizes, normaly of error terms plays a major role in the theory underlying prediction intervals. In fact, the width of these intervals (specifically the multipliers) is a direct result of normal theory --- the lack of normality means we are lost when searching for the correct multiplier (assuming the distribution of errors is unknown). Thus, even if the regression fit is solid, it may be worthwhile to check normality of errors --- prediction intervals may not be as trustworthy as they seem. And in the realm of housing prices, prediction intervals are extremely important --- it is common for someone to want to know what the price range of houses is for a given set of predictors. For instance, a family which needs a house with 3 bedrooms, at Z latitude, and with W square feet of living space needs to have a price range to adequately compare to similar houses available in another location.

The prevailing method for checking normality of errors is the normal probability plot, a special subtype of probability plot. In a normal probability plot (or qqplot), the residuals are plotted against their expected value under normality. That is, how likely a residual of that magnitude would be under the assumption that the residuals are normally distributed. The plot reads "quantiles," since this process implicitly compares the quantiles of the data (residuals) to the quantiles of a corresponding normal distribution (scaled by MSE). Thus, a plot that is linear suggests a normal distribution of errors, since each residual is nearly its expected value under the normality assumption. Furthermore, we can tell the exact manner in which the errors are distributed from this plot. For example, data which is skewed right would show up with a slope less than 1 for lower ordinal normal statistics (the x-axis) and greater than 1 for higher ordinal normal statistics. Data which has fat tails would show up at linear in the middle of the graph, but deviate to the bottom and top of the linear fit for low and high ordinal normal statistics, respectively. 

The function which maps residuals to their expected value under normality has been found to be (k-.375/n+.25)*sqrt(MSE), where k is the kth smallest residual, and n is the total number of residuals. Since sqrt(MSE) is simply a scaling factor here, it may be omitted in the calculation without altering the shape of the plot. Let's move on to some examples for our data. As standardized residuals are simply another scaling factor, they to do not affect the nature of the plots, and we will use those as our input in the plots below.

```{r, echo=FALSE}
housing_lm <- lm(price ~ sqft_living, data=housing2)
# untransformed model
augment(housing_lm) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")
qqnorm(housing_lm$residuals)
```
The above Q-Q plot is for the SLR model where price is predicted solely from square footage of living space. As shown by the non-linearity, normality of errors is not a great assumption to make here. The standardized residual plot shows many issues here on top of normality (it is severely heteroskedastic). In assignment 2, we used a power model (log transformation on both x and y) to get around these issues. Let's see what the Q-Q plot looks like in that arena.

```{r, echo=FALSE}
housing_lm3 <- lm(log(price) ~ log(sqft_living), data=housing2)
# transforming both X and Y using log (power model)
augment(housing_lm3) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")
# power model residual
qqnorm(housing_lm3$residuals)
```
Much better! The power model fixed the issues with the variance, but simultaneously fixed the issues with normality of errors. This is the model we'd move forward with in the SLR setting. Now let's check the final model we presented in the MLR setting, which regressed the log of price on square footage of living space, latitude, view, condition, and interactions between square footage of living space and view, view and latitude, and square footage of living space and condition.
```{r, echo=FALSE, warning=FALSE}
full.model <- lm(log(price) ~ (sqft_living + as.factor(condition) + as.factor(view) + lat + sqft_living*as.factor(condition) + sqft_living*as.factor(view) + as.factor(view)*lat), data=housing2)

augment(full.model) %>%
ggplot( aes(x=.fitted, y=.std.resid)) +
geom_point() +
geom_hline(yintercept = 0) + labs(x = "Fitted", y = "Std. Resid.", title = "Standardized Residual Plot")

qqnorm(full.model$residuals)
```
Again, there is nothing to worry about here. The Q-Q plot shows slight bias towards fat tails, but that's why we use the t-distribution anyways! We should note that we weren't expecting a major deviation from normality in an of these scenarios --- our sample size was large (1000) and the transformations made the linear model very appropriate.

# Inverse Predictions

Inverse predictions refer to the situation when a new value appears on the response, and this value is used to infer what the value(s) of the predictors are using the existing regression of Y on X. In the housing market, situtations like this arise all the time. It is not uncommon for potential buyers to browse by price (Y) only at first, and having a tool which can build out the details of the house would be very valuable (obviously this information would typically exist, but maybe the buyer is extremely lazy).

Regression models are of course deterministic in nature, and so inverses are easy to come by. Note that the inverse prediction is NOT attained by regressing X on Y (called inverse regression), as this solves a different problem entirely (albeit with a similar result in most cases). The estimated regression function is assumed to be as always:

Y_hat = b_0 + b_1*X

If we solve this equation for X given Y_new, we get the form

X_new_hat = (Y_new - b_0) / b_1

which is the MLE for X_new assuming that there is a linear relationship between X and Y (b_1 cannot be 0). Assuming normality of errors and constant variance as before grants the confidence interval:

X_new_hat Â± t(1 - alpha/2; n-2)s{predX}

where

s{predX} = MSE / b_1^2 [1 + (1/n) + ((X_new_hat - X_bar)^2 / sum (X_i - X_bar)^2)

And that's it. If we can safely conclude that beta_1 is NOT 0, then we are good to go with the inverse predictions. Note that the given interval above relies on normal theory --- an exact confidence interval may be obtained through some tedious algebra. It will not be reproduced here, but may be found at https://journal.r-project.org/archive/2014/RJ-2014-009/RJ-2014-009.pdf in the documentation for the "investr" package, which we will employ to get inversion intervals.

Furthermore, in the case of multiple new observations, a Bonferroni or Scheffe procedure may be specified to adjust the critical values of the intervals accordingly. This will be illustrated below. 

The model we will use is the same as above, where the log of price is regressed on the log of square footage of living space. The "investr" package was used to compute exact confidence intervals. This package automatically calculates the slope estimate, and performs a test of significance to make sure there is a linear relationship present. First, we will attempt to estimate what level of square footage of housing space led to a house listed for 500,000 dollars.

```{r, warning=FALSE, echo=FALSE}
library(investr)
housing_lm3 <- lm(log(price) ~ log(sqft_living), data=housing2)
res <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90)
cat("A 95% CI for the square footage of living space of a home listed for 500,000 dollars in King County is (",exp(res$lower),",",exp(res$upper),") sq. ft.")


ggplot(housing_pred, aes(x = exp(log.sqft_living.), y = exp(log.price.))) + geom_point(alpha = .5, col = 'black') +
stat_smooth(method = "lm", se = FALSE) + geom_hline(yintercept =  500000, lty = 2)+geom_vline(xintercept = c(exp(res$lower), exp(res$estimate), exp(res$upper)), lty = 2)+xlim(0,6000)+labs(x = "Square Footage of Living Space", y = "Price")
```

In the plot above, the shaded area represents the confidence interval around the fit of the regression function, while the dashed lines represent the new observation, the best guess for what value of square footage of living space corresponds to a 500,000 dollar house, and the inversion intervals for that guess at 90% confidence. Recall that the regression model was fit on the log of both predictor and response, so they have been back-transformed here.

```{r, echo=FALSE, warning=FALSE}
res2 <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90, adjust = "Scheffe", k = 5)
res3 <- calibrate(housing_lm3, y0 = log(500000), interval = "inversion", level = 0.90, adjust = "Bonferroni", k = 5)

ggplot(housing_pred, aes(x = exp(log.sqft_living.), y = exp(log.price.))) + geom_point(alpha = .5, col = 'black') +
stat_smooth(method = "lm", se = FALSE)+geom_hline(yintercept =  500000, lty = 2)+xlim(0,9000)+labs(x = "Square Footage of Living Space", y = "Price")+geom_vline(xintercept = c(exp(res2$lower), exp(res2$estimate), exp(res2$upper)), lty = 2, col="blue")+geom_vline(xintercept = c(exp(res3$lower), exp(res3$estimate), exp(res3$upper)), lty = 2, col = "red")+geom_vline(xintercept = c(exp(res$lower), exp(res$estimate), exp(res$upper)), lty = 2)
```

The above plot shows simultaneous inference for 5 new houses that entered the market at 500,000. In red are the Scheffe bounds, and in blue are the Bonferroni bounds for the inversion intervals. Note that all intervals here are big --- perhaps too big? We cannot determine if there is an error somewhere within the package or if the inversion intervals for simultaneous inference really are that large. Either way, nobody is going to be getting any information out of knowing that a house listed for 500,000 dollars is between 1000 and 45000 square feet.

# Generalized Additive Models
GAMs provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. And GAMs can be applied with both quantitative and qualitative responses.

An example of GAM would be, instead of having $y_i = \beta_0 + \beta_1 x_{i1}+ \beta_2x_{i2} + ... \beta_p x_{ip} + \epsilon_i$, we would write the model as  $y_i = \beta_0 + f_1(x_{i1})+ f_2(x_{i2}) + .... f_p(x_{ip}) + \epsilon_i$. It shows the addictive characteristic because we calculate a separate $f_j$ for each $X_j$, and then add together all of their contributions.

We picked GAMs because GAMs allow us to fit a non-linear $f_j$ to each $X_j$, so that we can automatically model non-linear relationships that standard linear regression will miss. And the non-linear fits can potentially make more accurate predictions. For example, for latitude, GAMs might yield better fit for us. 
Also, since the model is additive, we can still examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed. Hence if we are interested in inference, GAMs provide a useful representation.


```{r}
library(mgcv)
library(gam)
# fit the model using natural splines
gam1=lm(price ~ ns(sqft_living,4)+ns(lat,5)+ns(condition, 5),data=housing)
summary(gam1)
<<<<<<< HEAD
#plot(gam1)
par(mfrow=c(1,2))
plot(gam1, se=TRUE,col="black")
=======
par(mfrow=c(1,3))
plot.gam(gam1, se=TRUE,col="blue")
>>>>>>> d60c76de1bbfa6ac1669086237ed7ab41a7f3268

# fit the model using smoothing splines 
gam2=lm(price ~ s(sqft_living,4)+s(lat,5)+s(condition, 5),data=housing)
par(mfrow=c(1,3))
plot.gam(gam2, se=TRUE,col="blue")

# GAM that uses a linear function of latitude 
gam3=lm(price ~ ns(sqft_living,4)+lat +ns(condition, 5),data=housing)
# GAM that excludes latitude 
gam4=lm(price ~ ns(sqft_living,4)+ns(condition, 5),data=housing)
anova(gam1,gam3,gam4,test="F")
```
Each plot displays the fitted function and pointwise standard errors.
As we can see, there is a positive relationship between square feet living/price and condition/price. And houses with a latitude of 47.7 is more expensive. 
From the f-test we find that there is compelling evidence that a GAM with a non-linear function of latitude is better than a GAM with a linear function of latitude or one that does not latitude at all (p-value ~ 0). And this confirms our conjecture that non-linear fits could be better for latitude.

# Summary
